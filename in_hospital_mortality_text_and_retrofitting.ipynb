{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8elwvxUiHzt"
      },
      "source": [
        "The task in this notebook is mortality prediction on 48hrs using clinical notes.\n",
        "\n",
        "The positive class is mortality and the negative survive.\n",
        "\n",
        "We define preprocessing, and test, training classes in python and pytorch. You can use the preivous practical as an exmaple to complete this notebook\n",
        "\n",
        "# Exercises\n",
        "\n",
        "1. Check the file `test_text_data_2/train/20/20_notes.txt`\n",
        "   - Plot the bar char of the most top-35 most common words\n",
        "   - Which words do seem most relevant to you?\n",
        "1. For the mortality prediction model based on average word embeddings (Bag-of-Words, BoW)  \n",
        "  - Implement the missing layers in the constructor\n",
        "  - Implement the forward pass, note: the batch for loop is different, and you will need to call the function create sentence batch\n",
        "      - use mean of word embeddings to aggregate features for the classifier\n",
        "      - use average of the sentence (sum over the word dimension / sequence lenght) instead of mean to agregate features.  note: that the sequence lenght (seq_len) variable contains the actual size of each sentence in the batch. In addition, to define the correct size of the seq_len matrix you have to expand it with: `seq_len.unsqueeze(-1)`\n",
        "      - compare the AUC-ROC on test betweent the mean and average arregation\n",
        "  - Implement model selection\n",
        "  - Print the test results for all the metrics\n",
        "  - Plot AUC and calibration curves (for test data)\n",
        "  - run 3 random BoW with mean models (set seed to None) and report mean and std of the AUC-ROC and AUC-PR. what do you observe?\n",
        "\n",
        "2. Unfreeze the word embeddings for the BoW model with mean\n",
        "  - Compare the AUC-ROC on the test with the freeze model. what do you observe? Which one is better?\n",
        "  - Change the learning rate to 1e-4 for the unfreeze model and compare AUC-ROC with the previous models. what do you observe? Which one is better?\n",
        "\n",
        "3. Implement the LSTM model instead of BoW\n",
        "  - Add a LSTM layer into the constructor. NOTE: we recommend w_max is 3000 words, but you can try to push the GPU (also it will be slower). A longer sequence would make RNNs unfeasible.\n",
        "  - Use the best setting from the previous experiment for freeze or unfreeze word embeddings\n",
        "  - Apply the LSTM after the word embedding.\n",
        "  - Use mean as the aggeration of the LSTM ouputs.\n",
        "  - Print the test results for all the metrics\n",
        "  - Plot AUC and calibration curves (for test data)\n",
        "  - Compare the AUC-ROC on the test with the best BoW model. what do you observe? Which one is better?\n",
        "  - Use bidirectional LSTM\n",
        "    - Compare the AUC-ROC on the test with the LSTM. what do you observe? Which one is better?\n",
        "4. Compare the AUC-ROC across, your best structued, BoW and LSTM models. what do you observe? Which one is better?\n",
        "5. Train your own word embeddings with the extra glove example notebook.\n",
        "\n",
        "## Retrofitting exercises\n",
        "\n",
        "1.   Train word embeddings using retrofitting\n",
        "2.   Visualize retrofitted embedding\n",
        "  *   How the word2vec and retrofitted embeddings look like?\n",
        "  *   What difference do you see and why?\n",
        "3.   Train the model first using the word2vec embeddings and then retrofitted embeddings. Then evaluate both the trained models on the test set.\n",
        "  * Do you see any difference?\n",
        "  * Do results improve?\n",
        "  * Compare the AUC and the calibration curves (with and without retrofitting).\n",
        "  * plot AUC and calibration curves for both models (with and without retrofitting) together\n",
        "  * Try different (larger and smaller) values for the parameters ```-n``` of retrofitting\n",
        "      * Do the results change?\n",
        "4. Generate the lexicon for retrofitting using the additional notebook Named Entity Recognition with Scispacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bSMdxdvj2Zd"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ-CZ0Mph_pP"
      },
      "source": [
        "#dowload csv files from gdrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71EfVOVVV5j4"
      },
      "source": [
        "#download file from gdrive with the id for share file\n",
        "# create share link for tar.gz file  and copy id\n",
        "#https://drive.google.com/file/d/1E279yz7ZiZmok6qOYWlrWku1w7F13T5Q/view?usp=sharing\n",
        "file_id = '1S4jRAEmI4mLNCNhT3Z06bM9nhPSTSkyE' # URL id.\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('test_text_data_2.tar.gz')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4k9lBdhWTsH"
      },
      "source": [
        "#extract data\n",
        "!tar -xzf test_text_data_2.tar.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBABt4P-JUou"
      },
      "source": [
        "#download file with word embeddings\n",
        "# https://drive.google.com/file/d/1G9GRv2N4ZWZBnP22N-Y523HbQzO0Tek2/view?usp=drive_link\n",
        "file_id = '1G9GRv2N4ZWZBnP22N-Y523HbQzO0Tek2' # URL id.\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('mimic_vectors_training.100d.txt')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFVOrxW2gJ2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b8b756-9851-4a6a-884a-c172762b3e7f"
      },
      "source": [
        "#check size of vacabulary\n",
        "!wc -l mimic_vectors_training.100d.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130212 mimic_vectors_training.100d.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoiEq4_gZvVT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "7694ce66-e7f7-482b-8d69-82d1545d4b62"
      },
      "source": [
        "#download discretizer config\n",
        "#TODO add config into dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c348f56d-fd36-4656-a06d-21a850f772c7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c348f56d-fd36-4656-a06d-21a850f772c7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving discretizer_config.json to discretizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB4gprWnbcBg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "26f3dc48-c1c0-4bfa-ba37-c29f4c87a760"
      },
      "source": [
        "#download normalizer config\n",
        "#TODO add config into dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-389c9a1e-d1b3-4c44-9bc4-7e01b6f13689\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-389c9a1e-d1b3-4c44-9bc4-7e01b6f13689\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving norm_start_time_zero.normalizer to norm_start_time_zero.normalizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Upload retrofit.py**\n",
        "This file implements the retrofitting method seen in our class. More details and documentation are available [here](https://github.com/mfaruqui/retrofitting).\n",
        "\n",
        "NB: You need to convert the script from python2 to python3 before uploading it, since Google Colab no longer runs python2."
      ],
      "metadata": {
        "id": "HwqRJUml3xSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download retrofitting\n",
        "#TODO add config into dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "3_c-eI_Z3zWL",
        "outputId": "a98a8cd0-b44f-4968-d40e-65a669dfb199"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8325710f-364d-482f-9c7c-f4abc8fbc1c0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8325710f-364d-482f-9c7c-f4abc8fbc1c0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving retrofit.py to retrofit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lexicon (knowledge graph) for to use for your experiment** This is a lexicon based on [UMLS](https://www.nlm.nih.gov/research/umls/index.html). Specifically:\n",
        "  \n",
        "\n",
        "1.   Entities have been annotated from the clinical notes' of patients included in the training data;\n",
        "2.   Annotated entities have been linked to UMLS concepts;\n",
        "3.   Synonims from those UMLS concepts have been extracted (using the proper relation)."
      ],
      "metadata": {
        "id": "ruWsUeBJ37-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download umls synonyms config\n",
        "#TODO add config into dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ro0EGTCk4BQC",
        "outputId": "5bd1e201-1831-4420-896b-8464143a82e8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6399544e-a6b8-4fbf-bbc4-b683a2739c92\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6399544e-a6b8-4fbf-bbc4-b683a2739c92\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving lexicon_UMLS_synonyms.txt to lexicon_UMLS_synonyms.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLq6mf6OkPT8"
      },
      "source": [
        "## upload utils\n",
        "**Upload mimic__utils_text.py**\n",
        "For reading csv, normalize data, and imputation.\n",
        "imputation techinique is previous, there are other imputation methods avilable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGurxaL2jjH9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "563ad0bd-ef1f-4dce-dd28-bd10e7cb18f8"
      },
      "source": [
        "#download normalizer config\n",
        "#TODO add config into dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f4f2ab6-fa87-4973-adc4-277a3fcd1dae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f4f2ab6-fa87-4973-adc4-277a3fcd1dae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving mimic_utils_text.py to mimic_utils_text.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lnabeEAKay0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df49195a-f03f-44e1-e36b-8b25a302a259"
      },
      "source": [
        "# install stop words, you can use the option in the create batch funciton to filter  funcitonal words\n",
        "!pip install stop_words"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stop_words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop_words\n",
            "  Building wheel for stop_words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32896 sha256=b36d12872fc5c5828dd7946e0c5aefaa83b77ca62ea789c691b2d8723ab6164e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n",
            "Successfully built stop_words\n",
            "Installing collected packages: stop_words\n",
            "Successfully installed stop_words-2018.7.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRIHWUoPkVV2"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXZ7HWxhjeOD"
      },
      "source": [
        "#import python and pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import codecs\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import logging\n",
        "import tempfile\n",
        "import shutil\n",
        "import pickle\n",
        "import platform\n",
        "import json\n",
        "from datetime import datetime\n",
        "from nltk.corpus import stopwords\n",
        "from stop_words import get_stop_words\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import random\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "from sklearn import metrics\n",
        "from mimic_utils_text import InHospitalMortalityReader, Discretizer, Normalizer, read_chunk\n",
        "\n",
        "# Figures ROC and calibration curve\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "import matplotlib.transforms as mtransforms\n",
        "import pickle\n",
        "from sklearn import metrics"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run retrofit"
      ],
      "metadata": {
        "id": "1ZS9ThPsb3Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python retrofit.py"
      ],
      "metadata": {
        "id": "_IZRdL_mb5jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize embeddings\n",
        "Visualize word2vec using [T-distributed stochastic neighbor embedding (tSNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding).\n",
        "Vizualize also the retrofitted embeddings and  compare them with word2vec ones. How the word2vec and retrofitted embeddings look like? What difference do you see and why?"
      ],
      "metadata": {
        "id": "v9SLQ6fv4aKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load glove into gensim\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "glove_file = 'mimic_vectors_training.100d.txt'\n",
        "tmp_file = get_tmpfile(\"glove_word2vec.txt\")\n",
        "_ = glove2word2vec(glove_file, tmp_file)\n",
        "model = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t68biZ1E4e7Y",
        "outputId": "244f6bb6-863a-4070-dd59-5be393ddd7c0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-c0099bde3cca>:12: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  _ = glove2word2vec(glove_file, tmp_file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
        "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
        "        x = embeddings[:, 0]\n",
        "        y = embeddings[:, 1]\n",
        "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
        "        for i, word in enumerate(words):\n",
        "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
        "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
        "    plt.legend(loc=4)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    if filename:\n",
        "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#plot clusters in the same fig for word2vec\n",
        "keys = ['diabetes', 'sepsis', 'pneumonia']\n",
        "\n",
        "embedding_clusters = []\n",
        "word_clusters = []\n",
        "for word in keys:\n",
        "    embeddings = []\n",
        "    words = []\n",
        "    for similar_word, _ in model.most_similar(word, topn=20):\n",
        "        words.append(similar_word)\n",
        "        embeddings.append(model[similar_word])\n",
        "    embedding_clusters.append(embeddings)\n",
        "    word_clusters.append(words)\n",
        "\n",
        "embedding_clusters = np.array(embedding_clusters)\n",
        "n, m, k = embedding_clusters.shape\n",
        "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
        "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
        "\n",
        "tsne_plot_similar_words('Similar words from word2vec', keys, embeddings_en_2d, word_clusters, 0.7,\n",
        "                        'similar_words_word2vec.png')\n",
        "\n",
        "# Your code here\n",
        "# visualize retrofitted embeddings"
      ],
      "metadata": {
        "id": "2t8gKFUy4jJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S1iXwTTmr_l"
      },
      "source": [
        "## Pytorch Dataset\n",
        "\n",
        "We define a vocabulary, dataset, a collate function and create batch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6_0T4_z29-p"
      },
      "source": [
        "# vocabulary class to upload word2vec into pytorch\n",
        "# default tokens\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "SOS_TOKEN = \"<s>\"\n",
        "EOS_TOKEN = \"</s>\"\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "        Creates a vocabulary from a word2vec file.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "      # we add special tokens for padding, unknown word, start of sentence and end of sentence\n",
        "      # idx_to_word is a dictionary of ids to words\n",
        "      # word_to_idx is a dictionary from words to ids\n",
        "        self.idx_to_word = {0: PAD_TOKEN, 1: UNK_TOKEN, 2: SOS_TOKEN, 3: EOS_TOKEN}\n",
        "        self.word_to_idx = {PAD_TOKEN: 0, UNK_TOKEN: 1, SOS_TOKEN: 2, EOS_TOKEN: 3}\n",
        "        self.word_freqs = {}\n",
        "\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.word_to_idx[key] if key in self.word_to_idx else self.word_to_idx[UNK_TOKEN]\n",
        "\n",
        "    def word(self, idx):\n",
        "        return self.idx_to_word[idx]\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.word_to_idx)\n",
        "\n",
        "\n",
        "    def from_data(input_file, vocab_size, emb_size):\n",
        "      # we read the word embeddings file into memory\n",
        "      #and create the unknown word with the average of all the vocabulary\n",
        "        vocab = Vocabulary()\n",
        "        vocab_size = vocab_size + len(vocab.idx_to_word)\n",
        "        weight = np.zeros((vocab_size, emb_size))\n",
        "        with codecs.open(input_file, 'rb')  as f:\n",
        "\n",
        "          for l in f:\n",
        "            line = l.decode().split()\n",
        "            token = line[0] # first position token\n",
        "            if token not in vocab.word_to_idx:\n",
        "              idx = len(vocab.word_to_idx)\n",
        "              vocab.word_to_idx[token] = idx\n",
        "              vocab.idx_to_word[idx] = token\n",
        "\n",
        "              vect = np.array(line[1:]).astype(np.float) #all cols are the embeddings\n",
        "              weight[idx] = vect\n",
        "          # average embedding for unk word\n",
        "          avg_embedding = np.mean(weight, axis=0)\n",
        "          weight[1] = avg_embedding\n",
        "\n",
        "        return vocab, weight"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8JTbfcomYc4"
      },
      "source": [
        "\n",
        "# pytroch class for reading data into batches\n",
        "class MIMICTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "       Loads a list of sentences into memory from a text file,\n",
        "       split by newlines.\n",
        "    \"\"\"\n",
        "    def __init__(self, reader, discretizer, normalizer,\n",
        "            notes_output='sentence', max_w=25, max_s=500, max_d=500,\n",
        "            target_repl=False, batch_labels=False):\n",
        "        self.data = []\n",
        "        self.y  = []\n",
        "        self.max_w = max_w\n",
        "        self.max_s = max_s\n",
        "        self.max_d = max_d\n",
        "        N = reader.get_number_of_examples()\n",
        "        #if small_part:\n",
        "        #    N = 1000\n",
        "        ret = read_chunk(reader, N)\n",
        "        data = ret[\"X\"]\n",
        "        notes_text = ret[\"text\"] # we also load the text data from MIMIC\n",
        "        notes_info = ret[\"text_info\"] # extra info form the text like category of a note\n",
        "        ts = ret[\"t\"]\n",
        "        labels = ret[\"y\"]\n",
        "        names = ret[\"name\"]\n",
        "        data = [discretizer.transform(X, end=t)[0] for (X, t) in zip(data, ts)]\n",
        "        if normalizer is not None:\n",
        "            data = [normalizer.transform(X) for X in data]\n",
        "\n",
        "        #notes into list of sentences, docs, etc..\n",
        "        self.notes = []\n",
        "        tmp_data = []\n",
        "        tmp_labels = []\n",
        "        if notes_output == 'sentence':\n",
        "            # [N, W] patients, words\n",
        "            # we exclude patients that have more tan max_w words\n",
        "            for patient_notes, _x, l  in zip(notes_text, data, labels):\n",
        "                tmp_notes = []\n",
        "                for doc in sorted(patient_notes):\n",
        "                    sentences = patient_notes[doc]\n",
        "                    for sentence in sentences:\n",
        "                        #print(sentence)\n",
        "                        tmp_notes.extend(sentence)\n",
        "                if len(tmp_notes) > 0 and len(tmp_notes) <= self.max_w:\n",
        "                    #print(tmp_notes)\n",
        "                    self.notes.append(' '.join(tmp_notes))\n",
        "                    #self.notes.append(tmp_notes)\n",
        "                    tmp_data.append(_x)\n",
        "                    tmp_labels.append(l)\n",
        "                #elif len(tmp_notes) > 0:\n",
        "                #    self.notes.append(' '.join(tmp_notes[:self.max_w]))\n",
        "                #    tmp_data.append(_x)\n",
        "        elif notes_output == 'sentence-max':\n",
        "             # [N, W] patients, words\n",
        "             # we cut notes of each patient up to max_w words\n",
        "            for patient_notes, _x, l  in zip(notes_text, data, labels):\n",
        "                tmp_notes = []\n",
        "                for doc in sorted(patient_notes):\n",
        "                    sentences = patient_notes[doc]\n",
        "                    for sentence in sentences:\n",
        "                        #print(sentence)\n",
        "                        tmp_notes.extend(sentence)\n",
        "                if len(tmp_notes) > 0 and len(tmp_notes) <= self.max_w:\n",
        "                    #print(tmp_notes)\n",
        "                    self.notes.append(' '.join(tmp_notes))\n",
        "                    #self.notes.append(tmp_notes)\n",
        "                    tmp_data.append(_x)\n",
        "                    tmp_labels.append(l)\n",
        "                elif len(tmp_notes) > 0:\n",
        "                    self.notes.append(' '.join(tmp_notes[:self.max_w]))\n",
        "                    tmp_data.append(_x)\n",
        "                    tmp_labels.append(l)\n",
        "\n",
        "        elif notes_output == 'doc':\n",
        "            # [N, S, W] patients, sentences, words\n",
        "            # we cut notes into max sentences and each sentence into max words\n",
        "            for patient_notes,  _x, l in zip(notes_text, data, labels):\n",
        "                tmp_notes = []\n",
        "                for doc in sorted(patient_notes):\n",
        "                    sentences = patient_notes[doc]\n",
        "                    for sentence in sentences:\n",
        "                        if len(sentence) > 0 and len(sentence) <= max_w:\n",
        "                            tmp_notes.append(sentence)\n",
        "                        elif len(sentence) > 0:\n",
        "                            tmp_notes.append(sentence[:max_w])\n",
        "                if len(tmp_notes) > 0 and len(tmp_notes) <= max_s:\n",
        "                    self.notes.append(tmp_notes)\n",
        "                    tmp_data.append(_x)\n",
        "                    tmp_labels.append(l)\n",
        "                elif len(tmp_notes) > 0:\n",
        "                    self.notes.append(tmp_notes[:max_s])\n",
        "                    tmp_data.append(_x)\n",
        "                    tmp_labels.append(l)\n",
        "\n",
        "#\n",
        "        self.x = np.array(tmp_data, dtype=np.float32)\n",
        "        self.T = self.x.shape[1]\n",
        "        if batch_labels:\n",
        "            self.y = np.array([[l] for l in tmp_labels], dtype=np.float32)\n",
        "        else:\n",
        "            self.y = np.array(tmp_labels, dtype=np.float32)\n",
        "\n",
        "\n",
        "    def _extend_labels(self, labels):\n",
        "        # (B,)\n",
        "        labels = labels.repeat(self.T, axis=1)  # (B, T)\n",
        "        return labels\n",
        "\n",
        "    def __len__(self):\n",
        "        # overide len to get number of instances\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get words and label for a given instance index\n",
        "        # note now we have 2 sources or modalities of data\n",
        "        # structured variables, text and labels\n",
        "        return self.x[idx], self.notes[idx], self.y[idx]\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc8Qm4WA3QYT"
      },
      "source": [
        "#collate and create batch for sentences\n",
        "\n",
        "# the following function takes a batch of words [B, W] and creates a batch to the GPU\n",
        "# the batch is transfomred into ids and padded\n",
        "def create_sentence_batch(sentences, vocab, device, stopwords=False):\n",
        "    \"\"\"\n",
        "    Converts a list of sentences to a padded batch of word ids. Returns\n",
        "    an input batch, output tags, a sequence mask over\n",
        "    the input batch, and a tensor containing the sequence length of each\n",
        "    batch element.\n",
        "    :param sentences: a list of sentences, each a list of token ids\n",
        "    :param vocab: a Vocabulary object for this dataset\n",
        "    :param device:\n",
        "    :returns: a batch of padded inputs,  mask, lengths\n",
        "    \"\"\"\n",
        "    # we can skip stop words from a sequence\n",
        "    if stopwords:\n",
        "        tok = np.array([_remove_stopwords(sen) for sen in sentences])\n",
        "    else:\n",
        "        tok = np.array([sen.split() for sen in sentences])\n",
        "    #tok = np.array([sen[0] for sen in sentences])\n",
        "    seq_lengths = [len(sen) for sen in tok]\n",
        "    max_len = max(seq_lengths)\n",
        "    pad_id = vocab[PAD_TOKEN]\n",
        "    unk_id = vocab[UNK_TOKEN]\n",
        "\n",
        "    pad_id_input = []\n",
        "    #pad and find ids for words given the word2vec vocab\n",
        "    #print(tok)\n",
        "    for idx, sen in enumerate(tok):\n",
        "      tmp_sent = []\n",
        "      for t in range(max_len):\n",
        "        if t < seq_lengths[idx]:\n",
        "          try:\n",
        "            token_id = vocab[sen[t]]\n",
        "          except KeyError:\n",
        "            token_id = unk_id\n",
        "        else:\n",
        "          token_id = pad_id\n",
        "        tmp_sent.append(token_id)\n",
        "      pad_id_input.append(tmp_sent)\n",
        "\n",
        "\n",
        "    # Convert everything to PyTorch tensors.\n",
        "    batch_input = torch.tensor(pad_id_input)\n",
        "    seq_mask = (batch_input != vocab[PAD_TOKEN])\n",
        "    seq_length = torch.tensor(seq_lengths)\n",
        "\n",
        "    # Move all tensors to the given device.\n",
        "    batch_input = batch_input.to(device)\n",
        "    seq_mask = seq_mask.to(device)\n",
        "    seq_length = seq_length.to(device)\n",
        "\n",
        "    return batch_input, seq_mask, seq_length\n",
        "\n",
        "\n",
        "\n",
        "def doc_collate(batch):\n",
        "    data = np.array([item[0] for item in batch])\n",
        "    data = torch.tensor(data)\n",
        "    notes = [item[1] for item in batch]\n",
        "    target = np.array([item[2] for item in batch])\n",
        "    target = torch.tensor(target)\n",
        "    #target = torch.LongTensor(target)\n",
        "    return [data, notes, target]\n",
        "\n",
        "\n",
        "def create_doc_batch(docs, vocab, device):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    sent_seq_lengths = np.array([len(doc) for doc in docs])\n",
        "    word_seq_lengths = [[len(sent) for sent in doc] for doc in docs]\n",
        "    b = len(docs)\n",
        "    sent_max_len = max(sent_seq_lengths)\n",
        "    word_max_len = max([max(w_seq) for w_seq in word_seq_lengths])\n",
        "    pad_id = vocab[PAD_TOKEN]\n",
        "    unk_id = vocab[UNK_TOKEN]\n",
        "\n",
        "    pad_id_input = np.zeros((b, sent_max_len, word_max_len), dtype=int)\n",
        "    word_seq_length = np.ones((b, sent_max_len), dtype=np.float32)\n",
        "    for i, w_lens in enumerate(word_seq_lengths):\n",
        "        for j, w_len in enumerate(w_lens):\n",
        "            word_seq_length[i][j] = w_len\n",
        "    #pad and find ids for words given the word2vec vocab\n",
        "    #print(tok)\n",
        "    for idx_doc, doc in enumerate(docs):\n",
        "        #tmp_doc = []\n",
        "        for i in range(sent_max_len):\n",
        "            tmp_sent = []\n",
        "            if i < sent_seq_lengths[idx_doc]:\n",
        "                sent = doc[i]\n",
        "                for j in range(word_max_len):\n",
        "                    if j < word_seq_lengths[idx_doc][i]:\n",
        "                        try:\n",
        "                            token_id = vocab[sent[j]]\n",
        "                        except KeyError:\n",
        "                            token_id = unk_id\n",
        "                    else:\n",
        "                        token_id = pad_id\n",
        "                    #tmp_sent.append(token_id)\n",
        "                    pad_id_input[idx_doc][i][j] = token_id\n",
        "            else:\n",
        "                #tmp_sent = [pad_id for _ in range(word_max_len[idx_doc])]\n",
        "                for j in range(word_max_len):\n",
        "                    pad_id_input[idx_doc][i][j] = pad_id\n",
        "        #pad_id_input.append(tmp_sent)\n",
        "\n",
        "    # Convert everything to PyTorch tensors.\n",
        "    batch_input = torch.tensor(pad_id_input)\n",
        "    #seq_mask = (batch_input != vocab[PAD_TOKEN])\n",
        "    sent_seq_length = torch.tensor(sent_seq_lengths)\n",
        "    word_seq_length = torch.tensor(word_seq_length)\n",
        "\n",
        "    # Move all tensors to the given device.\n",
        "    batch_input = batch_input.to(device)\n",
        "    #seq_mask = seq_mask.to(device)\n",
        "    sent_seq_length = sent_seq_length.to(device)\n",
        "    word_seq_length = word_seq_length.to(device)\n",
        "\n",
        "    return batch_input, sent_seq_length, word_seq_length"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ikv2wdqEmxrZ"
      },
      "source": [
        "## Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qVNiqW1nJa7"
      },
      "source": [
        "# eval metrics\n",
        "def print_metrics_binary(y_true, predictions, logging, verbose=1):\n",
        "    predictions = np.array(predictions)\n",
        "    if len(predictions.shape) == 1:\n",
        "        predictions = np.stack([1 - predictions, predictions]).transpose((1, 0))\n",
        "    cf = metrics.confusion_matrix(y_true, predictions.argmax(axis=1))\n",
        "    if verbose:\n",
        "        logging.info(\"confusion matrix:\")\n",
        "        logging.info(cf)\n",
        "    cf = cf.astype(np.float32)\n",
        "\n",
        "    acc = (cf[0][0] + cf[1][1]) / np.sum(cf)\n",
        "    prec0 = cf[0][0] / (cf[0][0] + cf[1][0])\n",
        "    prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
        "    rec0 = cf[0][0] / (cf[0][0] + cf[0][1])\n",
        "    rec1 = cf[1][1] / (cf[1][1] + cf[1][0])\n",
        "    auroc = metrics.roc_auc_score(y_true, predictions[:, 1])\n",
        "\n",
        "    (precisions, recalls, thresholds) = metrics.precision_recall_curve(y_true, predictions[:, 1])\n",
        "    auprc = metrics.auc(recalls, precisions)\n",
        "    minpse = np.max([min(x, y) for (x, y) in zip(precisions, recalls)])\n",
        "\n",
        "    if verbose:\n",
        "        logging.info(\"accuracy = {0:.3f}\".format(acc))\n",
        "        logging.info(\"precision class 0 = {0:.3f}\".format(prec0))\n",
        "        logging.info(\"precision class 1 = {0:.3f}\".format(prec1))\n",
        "        logging.info(\"recall class 0 = {0:.3f}\".format(rec0))\n",
        "        logging.info(\"recall class 1 = {0:.3f}\".format(rec1))\n",
        "        logging.info(\"AUC of ROC = {0:.3f}\".format(auroc))\n",
        "        logging.info(\"AUC of PRC = {0:.3f}\".format(auprc))\n",
        "\n",
        "\n",
        "    return {\"acc\": acc,\n",
        "            \"prec0\": prec0,\n",
        "            \"prec1\": prec1,\n",
        "            \"rec0\": rec0,\n",
        "            \"rec1\": rec1,\n",
        "            \"auroc\": auroc,\n",
        "            \"auprc\": auprc}"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j0H4HwTCx5U"
      },
      "source": [
        "## Understand the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMjuNAkNC0gL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b77e1cf-1802-431f-858b-51ab1cb158b8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# read the file\n",
        "with open('test_text_data_2/train/20/20_note.txt','r') as f:\n",
        "  txt = f.read()\n",
        "# Exclude headers\n",
        "txt = re.sub(\"[0-9]+,[0-9]+,[0-9][0-9][0-9][0-9]-[0-1][0-9]-[0-3][0-9]\\s[0-2][0-9]:[0-5][0-9]:[0-9][0-9],[0-9][0-9][0-9][0-9]-[0-1][0-9]-[0-9][0-9]\\s[0-9][0-9]:[0-9][0-9]:[0-9][0-9],[0-9]+\\.[0-9][0-9]+,[0-9]+,[0-9]+\\.[0-9]+\\n\", \"\", txt)\n",
        "# tokenize\n",
        "txt = txt.split('\\n')\n",
        "# exclude puctuation\n",
        "txt = filter(lambda x: not re.match(r'^,|:|to|%|\\.|\\*+|\\[|\\]|\\(|\\)|\\^|\\s*$', x), txt)\n",
        "# exclude stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_sentence = [w for w in txt if not w.lower() in stop_words]\n",
        "# Your code here\n",
        "# Hint: you can use nltk.FreqDist and nltk.FreqDist.plot()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2B6Q5Wwkvgk"
      },
      "source": [
        "## Training\n",
        "\n",
        "Here we define the model, loss and training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pye3W2HkjoOp"
      },
      "source": [
        "#model Bow with a single sequence for a patient\n",
        "\n",
        "class BoWText(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, label_size, emb_size, hidden_size, dropout=0.2, model_w2vec=None, bidirectional=False):\n",
        "\n",
        "        super().__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        # This is how the network uploads embeddings into the lookup layer\n",
        "        # numpy vectors into tensor\n",
        "        weights = torch.FloatTensor(model_w2vec)\n",
        "        # this embedder will be the encoder for the text notes\n",
        "        # freeze true means that is not going to update the parameters of the word embeddings\n",
        "        # experiment: set freeze to True/False (because Glove is general and our data is domain specific)\n",
        "        self.embedder = nn.Embedding.from_pretrained(weights, freeze=True)\n",
        "        # your code here\n",
        "        # add hidden combination layer\n",
        "        # add projection layer\n",
        "        # add dropout layer\n",
        "        # add relu activation function\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, seq_mask, seq_len): #x=notes\n",
        "        # your code here\n",
        "        # Compute word embeddings\n",
        "        # [B,W,E] B=patient, M=sentence, E=embedings\n",
        "        # apply dropout\n",
        "        # apply a hidden layer\n",
        "        # [B, W, hid_size]\n",
        "        # apply mean to aggregate features\n",
        "        # project features into logits\n",
        "        # [B,1]\n",
        "\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG63f4LBnFFf"
      },
      "source": [
        "#eval model\n",
        "def eval_model(model, dataset, device, vocab):\n",
        "    model.eval()\n",
        "    sigmoid = nn.Sigmoid()\n",
        "    with torch.no_grad():\n",
        "        y_true = []\n",
        "        predictions = []\n",
        "        for _, notes, labels  in dataset:\n",
        "            labels = labels.to(device)\n",
        "            x_notes, seq_mask, seq_len = create_sentence_batch(notes,\n",
        "                    vocab,\n",
        "                    device,\n",
        "                    stopwords=False)\n",
        "\n",
        "            logits =  model(x_notes, seq_mask, seq_len)\n",
        "            probs = sigmoid(logits)\n",
        "            #_, predicted = torch.max(probs.data, 1)\n",
        "            #y_hat_class = np.where(probs.data<0.5, 0, 1)\n",
        "            predictions += [p.item() for p in probs]#y_hat_class.squeeze()\n",
        "            y_true += [y.item() for y in labels]\n",
        "    #print(predictions)\n",
        "    #print(y_true)\n",
        "    results = print_metrics_binary(y_true, predictions, logging)\n",
        "    return results, predictions, y_true"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WSyj4Minlc6"
      },
      "source": [
        "def train(args):\n",
        "  mode = 'train'\n",
        "  hidden_size = args['dim']\n",
        "  dropout = args['dropout']\n",
        "  batch_size = args['batch_size']\n",
        "  learning_rate = args['lr']\n",
        "  num_epochs = args['epochs']\n",
        "  emb_size = args['emb_size']\n",
        "  aggregation_type = args['aggregation_type']\n",
        "  bidirectional_encoder = args['bidirectional'] # TODO add into args\n",
        "  seed = args['seed']\n",
        "  steps = args['steps']\n",
        "  data = args['data']\n",
        "  notes = args['notes']\n",
        "  word2vec = args['word2vec']\n",
        "  max_w = args['max_w']\n",
        "  timestep = args['timestep']\n",
        "  vocab_size = args['vocab_size']\n",
        "  normalizer_state = args['normalizer_state']\n",
        "\n",
        "\n",
        "  if seed:\n",
        "      torch.manual_seed(seed)\n",
        "      np.random.seed(seed)\n",
        "  device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "  logging.basicConfig(level=logging.INFO,\n",
        "          format='%(asctime)s %(message)s',\n",
        "          datefmt='%Y-%m-%d %H:%M:%S',\n",
        "          )\n",
        "\n",
        "  #vocab, weight = Vocabulary.from_data(word2vec, 130212, emb_size) #glove size vocabulary, emb size\n",
        "  vocab, weight = Vocabulary.from_data(word2vec, vocab_size, emb_size) #glove size vocabulary, emb size\n",
        "  print(vocab)\n",
        "    #rint(vocab[\"<unk>\"])\n",
        "\n",
        "  train_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data, 'train'),\n",
        "                                        notes_dir=notes,\n",
        "                                        listfile=os.path.join(data, 'train_listfile.csv'),\n",
        "                                         period_length=48.0)\n",
        "\n",
        "  val_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data, 'train'),\n",
        "                                       notes_dir=notes,\n",
        "                                       listfile=os.path.join(data, 'val_listfile.csv'),\n",
        "                                       period_length=48.0)\n",
        "\n",
        "  discretizer = Discretizer(timestep=float(timestep),\n",
        "                          store_masks=True,\n",
        "                          impute_strategy='previous',\n",
        "                          start_time='zero')\n",
        "  discretizer_header = discretizer.transform(train_reader.read_example(0)[\"X\"])[1].split(',')\n",
        "  cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
        "\n",
        "  normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
        "  normalizer_state = normalizer_state\n",
        "  if normalizer_state is None:\n",
        "      normalizer_state = 'norm_start_time_zero.normalizer'\n",
        "\n",
        "  normalizer.load_params(normalizer_state)\n",
        "\n",
        "  # sentence-max option proces notes into single sequence\n",
        "  train_dataset = MIMICTextDataset(train_reader,\n",
        "                discretizer,\n",
        "                normalizer,\n",
        "                batch_labels=True,\n",
        "                max_w=max_w,\n",
        "                notes_output='sentence-max')\n",
        "  train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  val_dataset = MIMICTextDataset(val_reader,\n",
        "            discretizer,\n",
        "            normalizer,\n",
        "            batch_labels=True,\n",
        "            max_w=max_w,\n",
        "            notes_output='sentence-max')\n",
        "  val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  # Define the classification model.\n",
        "  model = BoWText(vocab_size=vocab.size(),\n",
        "                        label_size=1,\n",
        "                        emb_size=emb_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        dropout=dropout,\n",
        "                        model_w2vec=weight)\n",
        "\n",
        "  model = model.to(device)\n",
        "  logging.info(args)\n",
        "  logging.info(model)\n",
        "\n",
        "  # Define optimizer\n",
        "  optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  # path to best model save on disk\n",
        "  best_model = 'best_model.pt'\n",
        "  best_val_auc = 0.\n",
        "\n",
        "  results = []\n",
        "\n",
        "\n",
        "  step = 0\n",
        "  num_batches = 0\n",
        "  # your code here\n",
        "  #training loop\n",
        "  # loop over the epochs\n",
        "  #for epoch_num...\n",
        "      #loss_batch = .0\n",
        "      #num_batches = 0\n",
        "      # loop over mini-batches note now we ignore x and use notes as input to create batch\n",
        "      #for _, notes, labels  in train_dl:\n",
        "          # use create sentence batch to make batch tensors for input for your model\n",
        "          #x_notes, seq_mask, seq_len = create_sentence_batch(notes,\n",
        "          #          vocab,\n",
        "          #          device,\n",
        "          #          stopwords=False)\n",
        "\n",
        "          # run forward\n",
        "          # Backpropagate and update the model weights.\n",
        "\n",
        "\n",
        "          # Every x steps we evaluate the model and report progress.\n",
        "          if step % steps == 0:\n",
        "              logging.info(\"epoch (%d) step %d: training loss = %.2f\"%\n",
        "                 (epoch_num, step, loss_batch/num_batches))\n",
        "\n",
        "\n",
        "          step += 1\n",
        "\n",
        "\n",
        "      metrics_results, _, _ = eval_model(model,\n",
        "                                    val_dl,\n",
        "                                    device,\n",
        "                                    vocab)\n",
        "      metrics_results['epoch'] = epoch_num\n",
        "      results.append(metrics_results)\n",
        "      #Your code here\n",
        "      # model selection\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbOmZFUhsARx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8f9bc1-8376-4d8a-bfb5-51fca7a76ec6"
      },
      "source": [
        "#execute training\n",
        "#define hyperparameters\n",
        "word2vec_fname = 'mimic_vectors_training.100d.txt'\n",
        "vocab_size = 130212\n",
        "\n",
        "args = {'dim':128,\n",
        "        'dropout':0.2,\n",
        "        'batch_size':64,\n",
        "        'lr':1e-3,\n",
        "        'epochs':1,\n",
        "        'emb_size':100, # from our pretrained embeddings\n",
        "        'aggregation_type':'mean',\n",
        "        'bidirectional':False,\n",
        "        'seed':42, # set to None for random model\n",
        "        'steps':50,\n",
        "        'data':'test_text_data_2/in-hospital-mortality',\n",
        "        'notes': 'test_text_data_2/train',\n",
        "        'word2vec': word2vec_fname, #file with word embeddings\n",
        "        'max_w': 10000, # note change for LSTM and BiLSTM models\n",
        "        'timestep':1.0,\n",
        "        'imputation':'previous',\n",
        "        'normalizer_state':None,\n",
        "        'vocab_size': vocab_size}\n",
        "train(args)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-6268746fd511>:48: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  vect = np.array(line[1:]).astype(np.float) #all cols are the embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.Vocabulary object at 0x7ba98b88bbe0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-08c275eb6afe>:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  tok = np.array([sen.split() for sen in sentences])\n",
            "<ipython-input-43-9716aae11097>:14: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYLLKMUtk4zz"
      },
      "source": [
        "## Test\n",
        "\n",
        "Here we use the best validation model and run in test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G79PLqTHjy9p"
      },
      "source": [
        "#test\n",
        "\n",
        "def test(args):\n",
        "    # define trainning and validation datasets\n",
        "    mode = 'test'\n",
        "    hidden_size = args['dim']\n",
        "    dropout = args['dropout']\n",
        "    batch_size = args['batch_size']\n",
        "    emb_size = args['emb_size']\n",
        "    best_model = args['best_model']\n",
        "    data = args['data']\n",
        "    notes = args['notes']\n",
        "    word2vec = args['word2vec']\n",
        "    max_w = args['max_w']\n",
        "    timestep = args['timestep']\n",
        "    aggregation_type = args['aggregation_type']\n",
        "    bidirectional_encoder = args['bidirectional']\n",
        "    vocab_size = args['vocab_size']\n",
        "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    # 1. Get a unique working directory\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO,\n",
        "            format='%(asctime)s %(message)s',\n",
        "            datefmt='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    #vocab, weight = Vocabulary.from_data(word2vec, 130212, emb_size) #glove\n",
        "    vocab, weight = Vocabulary.from_data(word2vec, vocab_size, emb_size) #glove\n",
        "\n",
        "    test_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data, 'test'),\n",
        "                                         listfile=os.path.join(data, 'test_listfile.csv'),\n",
        "                                         notes_dir=notes,\n",
        "                                         period_length=48.0)\n",
        "\n",
        "\n",
        "    discretizer = Discretizer(timestep=float(timestep),\n",
        "                          store_masks=True,\n",
        "                          impute_strategy='previous',\n",
        "                          start_time='zero')\n",
        "\n",
        "    discretizer_header = discretizer.transform(test_reader.read_example(0)[\"X\"])[1].split(',')\n",
        "    cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
        "\n",
        "    normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
        "    normalizer_state = args['normalizer_state']\n",
        "    if normalizer_state is None:\n",
        "        #normalizer_state = 'ihm_ts{}.input_str:{}.start_time:zero.normalizer'.format(args.timestep, args.imputation)\n",
        "        #normalizer_state = os.path.join(os.path.dirname(__file__), normalizer_state)\n",
        "        normalizer_state = 'norm_start_time_zero.normalizer'\n",
        "    normalizer.load_params(normalizer_state)\n",
        "\n",
        "    # Read data\n",
        "    #vocab, weight = Vocabulary.from_data(word2vec, 107647, emb_size) #glove 107647 , 300)\n",
        "    vocab, weight = Vocabulary.from_data(word2vec, vocab_size, emb_size) #glove 107647 , 300)\n",
        "\n",
        "    # sentence option proces notes into single sequences\n",
        "    test_dataset = MIMICTextDataset(test_reader,\n",
        "            discretizer,\n",
        "            normalizer,\n",
        "            batch_labels=True,\n",
        "            max_w=max_w,\n",
        "            notes_output='sentence-max')\n",
        "\n",
        "    test_dl =  DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "    # Define the classification model.\n",
        "    model = BoWText(vocab_size=vocab.size(),\n",
        "                        label_size=1, #label size = 1 because of the binary nature of the predictions(classification)\n",
        "                        emb_size=emb_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        dropout=dropout,\n",
        "                        model_w2vec=weight)\n",
        "\n",
        "    model.load_state_dict(torch.load(best_model))\n",
        "    logging.info(model)\n",
        "    model = model.to(device)\n",
        "\n",
        "    metrics_results, pred_probs, y_true = eval_model(model,\n",
        "                                test_dl,\n",
        "                                device,\n",
        "                                vocab)\n",
        "    return metrics_results, pred_probs, y_true\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-e2oIZ8qMP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f079ea-e331-4862-87e1-ae0979af35ee"
      },
      "source": [
        "word2vec_fname = 'mimic_vectors_training.100d.txt'\n",
        "vocab_size = 130212\n",
        "\n",
        "# Run test on best validation model\n",
        "# note use same hyper parameters as in training\n",
        "args = {'best_model':'best_model.pt',\n",
        "        'dim':128,\n",
        "        'dropout':0.2,\n",
        "        'batch_size':16,\n",
        "        'word2vec':word2vec_fname, #file with word embeddings\n",
        "        'emb_size':100,\n",
        "        'aggregation_type':'mean',\n",
        "        'bidirectional':False,\n",
        "        'data':'test_text_data_2/in-hospital-mortality',\n",
        "        'notes':'test_text_data_2/test',\n",
        "        'timestep':1.0,\n",
        "        'max_w':10000,\n",
        "        'imputation':'previous',\n",
        "        'normalizer_state':None,\n",
        "        'vocab_size': vocab_size}\n",
        "metrics_results, pred_probs, y_true = test(args)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-6268746fd511>:48: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  vect = np.array(line[1:]).astype(np.float) #all cols are the embeddings\n",
            "<ipython-input-42-08c275eb6afe>:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  tok = np.array([sen.split() for sen in sentences])\n",
            "<ipython-input-43-9716aae11097>:14: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMnyVBDXTGKG"
      },
      "source": [
        "## Plots\n",
        "\n",
        "ROC and calibrations curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1kDW3kqjgAf"
      },
      "source": [
        "# roc curve\n",
        "bow_fpr, bow_tpr, bow_thresholds = metrics.roc_curve(y_true, pred_probs)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.figure()\n",
        "plt.ylim(0., 1.0)\n",
        "plt.xlim(0.,1.0)\n",
        "plt.plot(bow_fpr, bow_tpr, marker='.', label='BoW', color='darkorange')\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "# show the legend\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C509pB4Eh7mA"
      },
      "source": [
        "#calibration curve\n",
        "bow_y, bow_x = calibration_curve(y_true, pred_probs, n_bins=10)\n",
        "plt.figure()\n",
        "plt.ylim(0., 1.0)\n",
        "plt.xlim(0.,1.0)\n",
        "#fig, ax = plt.subplots()\n",
        "# only these two lines are calibration curves\n",
        "plt.plot(bow_x,bow_y, marker='^', linestyle=\"\", markersize=7, label='BoW', color='darkorange')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "\n",
        "plt.xlabel('Mean predicted value')\n",
        "plt.ylabel('Fraction of positives')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H0b3_w0Hxw0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}